<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.9.1"/>
<title>Mechanics of Defect Evolution Library: DD on the Hoffman2 cluster</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Mechanics of Defect Evolution Library
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.9.1 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="_d_d_intro.html">Discrete Dislocation Dynamics (DDD)</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">DD on the Hoffman2 cluster </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a href="https://idre.ucla.edu/hoffman2">Hoffman2</a> is the high-performance computer cluster at <a href="http://www.ucla.edu">UCLA</a>. If you are using MODEL-DD on Hoffman2, this page may contain useful information about the use of MODEL.</p>
<h6></h6>
<h1><a class="anchor" id="hoffman2_login"></a>
Login and Setup</h1>
<pre class="fragment">ssh username@hoffman2.idre.ucla.edu
</pre><p>First request a c++11-capable compiler: </p><pre class="fragment">module load gcc/4.7.2
</pre><p> Then, if you are building or running the MPI version of the code you also need to load the MPI library built with gcc/4.7.2: </p><pre class="fragment">module load openmpi
</pre><p>Alternatively, you can add the following lines to your .bashrc file </p><pre class="fragment">module load gcc/4.7.2 &gt; /dev/null 2&gt;&amp;1 
module load openmpi &gt; /dev/null 2&gt;&amp;1 
</pre><h6></h6>
<h1><a class="anchor" id="hoffma2_dd_compile"></a>
Compiling</h1>
<p>Compile DDomp with </p><pre class="fragment">make DDomp
</pre><p>Compile DDmpi with </p><pre class="fragment">make DDmpi
</pre><p>See also the instructions in the tutorial <a class="el" href="_p_i_chargedparticles.html#FRsource_makefile">Compiling: the Makefile</a>.</p>
<h6></h6>
<h1><a class="anchor" id="hoffma2_dd_interactive"></a>
Running in interactive mode</h1>
<p>The command used to obtain an interactive section is <a href="http://hpc.ucla.edu/hoffman2/computing/sge_qrsh.php">qrsh</a>. The command qrsh is followed by the "-l" directive which allows to specify a series of parameters separated by commas. For example </p><pre class="fragment">qrsh -l h_data=4g,h_rt=2:00:00
</pre><p> requests one (1) interactive processor with 4Gb of memory (h_data=4g) for 2 hours, 0 minutes, and 0 seconds (h_rt=8:00:00). The maximum time limit is 24h, unless you are a member of a resource group. In that case the time limit is 14days, but the "highp" parameter must be used: </p><pre class="fragment">qrsh -l h_data=4g,h_rt=48:00:00,highp
</pre><p> If your request cannot be accommodated immediately but you are willing to wait indefinitely until it is eventually honored, you can append the flag "-now n" to the qrsh command: </p><pre class="fragment">qrsh -l h_data=4g,h_rt=48:00:00,highp -now n
</pre><p> The maximum wait time for members of resource groups is 24h.</p>
<p>If you issue one of the previous commands, however, you still ha have one (1) processors to work with. In order to request more than one processor, we need the "-pe" (parallel environment) directive. The way you use the "-pe" directive depends on the version of the DD code that we run, as explained below.</p>
<h6></h6>
<h2><a class="anchor" id="hoffma2_dd_interactive_DDomp"></a>
DDomp</h2>
<p>If you compiled DDomp, <a href="http://openmp.org">OpenMP</a> is used to speed up the most computationally-intensive loops. Therefore, in order to achieve optimum performance, DDomp must be run on a shared-memory machine with the largest possible number of cores. In order to obtain one such machine with the qrsh command, you can use the directive "-pe shared N", where "N" is the requested number of cores. For example: </p><pre class="fragment">qrsh -l h_data=4g,h_rt=4:00:00 -pe shared 8
</pre><p> requests 8 cores on the same node, which "share" the memory of that node. Note that Hoffman2 has 8-, 12-, and 16-core nodes.</p>
<p>After the node has been obtained, you can run DDomp as: </p><pre class="fragment">./DDomp
</pre><h6></h6>
<h2><a class="anchor" id="hoffma2_dd_interactive_DDmpi"></a>
DDmpi</h2>
<p>If you compiled DDmpi, <a href="http://www.open-mpi.org">Open-MPI</a> is used in combination with <a href="http://openmp.org">OpenMP</a> to speed up the most computationally-intensive loops. You can then request multiple nodes (machines) on the cluster which communicate using <a href="http://www.open-mpi.org">Open-MPI</a>, each of them internally using <a href="http://openmp.org">OpenMP</a>. Therefore, you must request "entire" nodes on the <a href="https://idre.ucla.edu/hoffman2">Hoffman2</a> cluster, where "entire" here that all cores on a specific machine will be reserved to run DDmpi. The parallel environment to be requested via qrsh is "-pe node*" in combination with the "exclusive" directive. For example: </p><pre class="fragment">qrsh -l h_data=4g,h_rt=3:00:00,exclusive,highp -now n -pe node* 4
</pre><p> requests 4 nodes, for 3 hours, and from the resource group (highp). If each node has 12 cores, the total number of cores used will be 4*12=48. DDmpi will then create 4 MPI processes (on for each node), each internally creating 12 threads.</p>
<p>After the nodes have been obtained, it is necessary to update the environmental variables. </p><pre class="fragment">. /u/local/bin/set_qrsh_env.sh
</pre><p>You can run DDmpi using the "mpirun" command. However, in order to make sure that the correct version of "mpirun" is used, it is recommended to specify its full path using the environmental variable $MPI_BIN: </p><pre class="fragment">$MPI_BIN/mpirun -pernode DDmpi
</pre><p> where "N" is the number of requested nodes. The "-pernode" flag is an alternative to using a hostfile.</p>
<h6></h6>
<h2><a class="anchor" id="hoffma2_dd_interactive_DDmpiOLD"></a>
DDmpi (OLD)</h2>
<p>If you compiled DDmpi, <a href="http://www.open-mpi.org">Open-MPI</a> is used to speed up the most computationally-intensive loops. Therefore, you can request processors belonging to different nodes using the "-pe dc_* N" directive, where N is the number of nodes. For example: </p><pre class="fragment">qrsh -l h_data=4g,h_rt=4:00:00 -pe dc_* 96
</pre><p> requests 96 processors. To use group resources (highp): </p><pre class="fragment">qrsh -l h_data=4g,h_rt=48:00:00,highp -now n -pe dc_* 96
</pre><p> request 96 processors of your resource group.</p>
<p>After the processors have been obtained, you can run DDmpi using the "mpirun" command. However, in order to make sure that the correct version of "mpirun" is used, it is recommended to specify its full path using the environmental variable $MPI_BIN: </p><pre class="fragment">$MPI_BIN/mpirun -np N DDmpi
</pre><p> where "N" is the number of requested processors.</p>
<h6></h6>
<h1><a class="anchor" id="hoffma2_dd_batch"></a>
Running in batch mode</h1>
<h6></h6>
<h2><a class="anchor" id="hoffma2_dd_batch_DDomp"></a>
DDomp</h2>
<p>TO BE COMPLETED </p><pre class="fragment">openmp.q
</pre><p> Answer the following questions</p><ul>
<li></li>
<li></li>
<li></li>
<li>answer NO to the</li>
</ul>
<p>Edit the file DDomp.cmd</p>
<p>add the line "module load gcc/4.7.2"</p>
<h6></h6>
<h2><a class="anchor" id="hoffma2_dd_batch_DDmpi"></a>
DDmpi</h2>
<p><a class="el" href="class_a.html">A</a> dedicated script has been created to run DDmpi on the Hoffman2 cluster in batch mode. The script, named submit_DDmpi.sh, is listed at the bottom of this page.</p>
<p>Before using the script, the following should be edited</p><ul>
<li>h_rt=HHH:MM:SS (the time requested for the run)</li>
<li>if only resource nodes are to be used, append the "highp" directive to the "-l" list</li>
</ul>
<p>Once the above options have been selected, use the script as follows: </p><pre class="fragment">qsub -pe node* 10 submit_DDmpi.sh
</pre><p> where, in this example, 10 is the number of requested nodes. Notice that by using "-pe node* 10" you are requesting 10 nodes on the cluster, each of them containing multiple cores. DDmpi will then create 10 mpi processes, each of them using openmp internally to fully take advantage of multithreading.</p>
<p>The job should now be visible using </p><pre class="fragment">myjob
</pre><p>The job can be deleted using </p><pre class="fragment">qdel &lt;jobID&gt;
</pre><p>Following is the submit_DDmpi.sh script (thanks Raffaella D'Auria): </p><div class="fragment"><div class="line">#!/bin/bash</div>
<div class="line"></div>
<div class="line"># change directory to work directory</div>
<div class="line">#$ -cwd</div>
<div class="line"></div>
<div class="line"># define job log file</div>
<div class="line">#$ -o parallel_job_penode.joblog.$JOB_ID</div>
<div class="line"></div>
<div class="line"># Merged job error file error   with job log</div>
<div class="line">#$ -j y</div>
<div class="line"></div>
<div class="line"># If number of nodes is not passed to qsub, uncomment following line and define number of nodes as needed (e.g. 5)</div>
<div class="line"># #$ -pe node* 5</div>
<div class="line"></div>
<div class="line"># Request resources per node:</div>
<div class="line"># h_data = minimum memory per core - use at least 4Gb</div>
<div class="line"># h_rt = time in HHH:MM:SS</div>
<div class="line"># exclusive = reserves the entire node (needed for openmp)</div>
<div class="line"># highp = add highp to run on sponsor nodes</div>
<div class="line">#$ -l h_data=4096M,h_rt=8:00:00,exclusive</div>
<div class="line"></div>
<div class="line">#  Email address to notify</div>
<div class="line">#$ -M $USER@mail</div>
<div class="line">#  Notify at beginning and end of job</div>
<div class="line">#$ -m bea</div>
<div class="line">#  Job is not rerunable</div>
<div class="line">#$ -r n</div>
<div class="line"></div>
<div class="line"># echo info on joblog:</div>
<div class="line">echo &quot;Job $JOB_ID started on:   &quot;` hostname -s `</div>
<div class="line">echo &quot;Job $JOB_ID started on:   &quot;` date `</div>
<div class="line">echo &quot; &quot;</div>
<div class="line">echo &quot;PE_HOSTFILE: &quot;</div>
<div class="line">cat $PE_HOSTFILE | awk &#39;{print $1&quot; &quot;$2}&#39;</div>
<div class="line"></div>
<div class="line"></div>
<div class="line"># set the environment</div>
<div class="line">. /u/local/Modules/default/init/modules.sh</div>
<div class="line">module load gcc/4.7.2  &gt; /dev/null 2&gt;&amp;1</div>
<div class="line">module load openmpi &gt; /dev/null 2&gt;&amp;1 </div>
<div class="line"></div>
<div class="line"># run the program</div>
<div class="line">$MPI_BIN/mpiexec -pernode ./DDmpi &gt;&amp; parallel_job_penode.output.$JOB_ID</div>
</div><!-- fragment --> </div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Mon Aug 3 2015 17:40:55 for Mechanics of Defect Evolution Library by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.9.1
</small></address>
</body>
</html>
