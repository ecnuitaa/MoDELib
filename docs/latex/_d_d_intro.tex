\hypertarget{_p_i_l_Introduction}{}\section{Introduction}\label{_p_i_l_Introduction}
\hypertarget{_p_i_l_Tutorials}{}\section{Tutorials}\label{_p_i_l_Tutorials}
The best way to learn how to use the Discrete Dislocation Dynamics module of M\+O\+D\+E\+L is to study the following tutorials\+:
\begin{DoxyItemize}
\item \hyperlink{FRsource}{Getting Started\+: a simple F\+R source}
\item \hyperlink{DD_visualization}{The Dislocation Dynamics Viewer}
\item F\+Rsource\+\_\+bnd
\item \hyperlink{hoffman2}{D\+D on the Hoffman2 cluster}
\end{DoxyItemize}\hypertarget{DD_intro.dox_Credits}{}\section{Credits}\label{DD_intro.dox_Credits}
\hypertarget{FRsource}{}\section{Getting Started\+: a simple F\+R source}\label{FRsource}
In this tutorial we cover the necessary steps to get a minimal D\+D simulation running with M\+O\+D\+E\+L. All files necessary to compile and run this tutorial are located in $<$\+M\+O\+D\+E\+L\+D\+I\+R$>$/tutorials/\+Dislocation\+Dynamics/\+F\+Rsource.\hypertarget{_f_rsource_FRsource_main}{}\subsection{The source file (main.\+cpp)}\label{_f_rsource_FRsource_main}
The source file is main.\+cpp. This file contains the following code\+: 
\begin{DoxyCodeInclude}
\textcolor{comment}{/* This file is part of MODEL, the Mechanics Of Defect Evolution Library.}
\textcolor{comment}{ *}
\textcolor{comment}{ * Copyright (C) 2012 by Giacomo Po <gpo@ucla.edu>}
\textcolor{comment}{ *}
\textcolor{comment}{ * MODEL is distributed without any warranty under the}
\textcolor{comment}{ * GNU General Public License (GPL) v2 <http://www.gnu.org/licenses/>.}
\textcolor{comment}{ */}

\textcolor{preprocessor}{#define \_MODEL\_NON\_SINGULAR\_DD\_ 1 }\textcolor{comment}{/* 1 = Cai's non-singular theory, 2 = Lazar's non-singular gradient
       theory */}\textcolor{preprocessor}{}

\textcolor{preprocessor}{#include <\hyperlink{_dislocation_network_8h}{model/DislocationDynamics/DislocationNetwork.h}>}

\textcolor{keyword}{using namespace }\hyperlink{namespacemodel}{model};

\textcolor{keywordtype}{int} \hyperlink{bench_2bary_search_2main_8cpp_a0ddf1224851353fc92bfbff6f499fa97}{main} (\textcolor{keywordtype}{int} argc, \textcolor{keywordtype}{char}* argv[])
\{
    \textcolor{comment}{// Create a DislocationNetwork object}
        
    \hyperlink{classmodel_1_1_dislocation_network}{DislocationNetwork<3,1,CatmullRom,16,UniformOpen>} DN(
      argc,argv);
    \textcolor{comment}{// Run the simulation}
    DN.runSteps();
    
    \textcolor{keywordflow}{return} 0;
\}

\end{DoxyCodeInclude}
 If you know a little C++, this code is pretty self-\/explanatory. After including the necessary headers, all we do in the main function is\+:
\begin{DoxyItemize}
\item create an instance, named D\+N, of type \hyperlink{classmodel_1_1_dislocation_network}{Dislocation\+Network} by specifying a series of template parameters. These parameter are\+:
\begin{DoxyItemize}
\item 3 is the dimensionality of the space (three dimensions)
\item 1 is the degree of parametric continuity at the Spline\+Node(s)\+: 0= position continuity, 1= position and tangent continuity, 2 = position, tangent, and curvature continuity.
\item \hyperlink{classmodel_1_1_catmull_rom}{Catmull\+Rom} is the type of splines used. Alternatively, you can use \hyperlink{classmodel_1_1_hermite}{Hermite} splines
\item centripetal is the type of spline parametrization used. Alternatively you can use uniform, or chordal
\item 16 means that sixteen quadrature points per \hyperlink{classmodel_1_1_dislocation_segment}{Dislocation\+Segment} are used for numerical integration
\item \hyperlink{structmodel_1_1_uniform_open}{Uniform\+Open} is the type of quadrature used. Alternatively you can use \hyperlink{structmodel_1_1_gauss_legendre}{Gauss\+Legendre}.
\end{DoxyItemize}
\item call the method \hyperlink{classmodel_1_1_dislocation_network_a3f9d89b936222954861d3366d7b971dd}{Dislocation\+Network\+::run\+Steps()} on D\+N. This runs a fixed number of simulations time steps defined by the variable {\bfseries Nsteps} in the input file {\bfseries D\+Dinput.\+txt} (see below). Alternatively you can run the simulation for a fixed amount of time (see \hyperlink{classmodel_1_1_dislocation_network_aeac382c9e8b6df7737f9fa3d9fe46413}{Dislocation\+Network\+::run\+Time()}). You can also call the two methods consecutively or place them in a more complex for loop.
\end{DoxyItemize}\hypertarget{_p_i_chargedparticles_FRsource_makefile}{}\subsection{Compiling\+: the Makefile}\label{_p_i_chargedparticles_FRsource_makefile}
If all necessary external libraries have been installed (see D\+D\+\_\+installation), compilation of the source code above can conveniently be done using the provided Makefile. In order to use the Makefile, first change the working directory of your terminal into the directory containing the files of this tutorial\+: \begin{DoxyVerb}cd <MODELDIR>/tutorials/DislocationDynamics/FRsource
\end{DoxyVerb}


The same Makefile can now be used to compile both the serial version and the parallel version (M\+P\+I) of the code. The serial version can be compiled executing the following command in your terminal\+: \begin{DoxyVerb}make serial
\end{DoxyVerb}
 This compiles the source code into the executable {\bfseries D\+Dserial}.

Similarly, The parallel version (M\+P\+I) of the code can be compiled using\+: \begin{DoxyVerb}make parallel 
\end{DoxyVerb}
 This creates the executable {\bfseries D\+Dparallel}. Compiling the parallel version requires that both \hyperlink{openmpi}{open-\/\+M\+P\+I} and \hyperlink{metis}{M\+E\+T\+I\+S} are correctly installed in your system.\hypertarget{_p_i_chargedparticles_FRsource_run}{}\subsection{Running D\+Dserial and D\+Dparallel}\label{_p_i_chargedparticles_FRsource_run}
The serial version of the code can be run executing\+: \begin{DoxyVerb}./DDserial
\end{DoxyVerb}


On the other hand, the parallel version of the code can be run executing\+: \begin{DoxyVerb}mpirun -np N DDparallel
\end{DoxyVerb}
 where {\bfseries N} is the number of processors to be used.

Are you curious to see what running the code produced? Then jump to the next tutorial, where you learn how to use \hyperlink{DD_visualization}{The Dislocation Dynamics Viewer}. Otherwise, just keep reading for more explanation.\hypertarget{_f_rsource_FRsource_input}{}\subsection{Input files}\label{_f_rsource_FRsource_input}
The first thing that a \hyperlink{classmodel_1_1_dislocation_network}{Dislocation\+Network} does is to read the some input files. These files are\+:
\begin{DoxyItemize}
\item the vertex file V/\+V\+\_\+0.\+txt
\item the edge file E/\+E\+\_\+0.\+txt
\item the configuration file D\+Dinput.\+txt
\end{DoxyItemize}\hypertarget{_f_rsource_FRsource_output}{}\subsection{Output files}\label{_f_rsource_FRsource_output}
In M\+O\+D\+E\+L, code outputs are highly customizable. However, two standard outputs are always generated because they are needed for visualization and restart purposes. These are the vertex files and the edge files. These files are stored in different folders (the \char`\"{}\+V\char`\"{} folder and the \char`\"{}\+E\char`\"{} folder, respectively), and are numbered according to the simulation time step. For example, the 114-\/th simulation time step generates the following files\+:
\begin{DoxyItemize}
\item V/\+V\+\_\+114.\+txt
\item E/\+E\+\_\+114.\+txt
\end{DoxyItemize}

These files have the exact same format of the files V/\+V\+\_\+0.\+txt and E/\+E\+\_\+0.\+txt that have been used as inputs. In fact, any output file can be used as an input file. To (re)start a simulation at a specific time step, say 114, change the parameter {\bfseries start\+At\+Time\+Step} in D\+Dinput.\+txt\+: \begin{DoxyVerb}startAtTimeStep = 114;
\end{DoxyVerb}


The frequency of output can easily be set by the variable {\bfseries output\+Frequency} in {\bfseries D\+Dinput.\+txt}. For example, setting \begin{DoxyVerb}outputFrequency=7;
\end{DoxyVerb}
 will force the code to output files every 7 time simulation steps.

Other standard output files can optionally be generated by turning on some flags in D\+Dinput.\+txt. These flags are\+: \begin{DoxyVerb}outputGlidePlanes=1;
outputSpatialCells=1;
outputPKforce=1;
\end{DoxyVerb}
 \hypertarget{DD_visualization}{}\section{The Dislocation Dynamics Viewer}\label{DD_visualization}
M\+O\+D\+E\+L provides a dedicated viewer to display the output of Dislocation Dynamics simulations. The viewer, based on \href{http://www.opengl.org}{\tt Open\+G\+L} and \href{http://www.opengl.org/resources/libraries/glut/}{\tt G\+L\+U\+T}, is extremely portable and should work on most architectures.\hypertarget{_d_d_visualization_DD_viewer_compiling}{}\subsection{Compiling the viewer}\label{_d_d_visualization_DD_viewer_compiling}
The same Makefile used to compile {\bfseries D\+Dserial} (or {\bfseries D\+Dparallel}) is also used to compile the Dislocation Dynamics viewer (D\+Dviewer). To do this simply issue the command\+: \begin{DoxyVerb}make viewer
\end{DoxyVerb}
 This creates the executable {\bfseries D\+Dviewer}, which you can run by calling\+: \begin{DoxyVerb}./DDviewer
\end{DoxyVerb}


{\bfseries D\+Dviewer} reads the output files produced by {\bfseries D\+Dserial}({\bfseries D\+Dparallel}). In fact, upon lunching the {\bfseries D\+Dviewer}, your terminal will display the output files available for visualization.\hypertarget{_d_d_visualization_DD_visualization_options}{}\subsection{Visualization Options}\label{_d_d_visualization_DD_visualization_options}
      \hypertarget{hoffman2}{}\section{D\+D on the Hoffman2 cluster}\label{hoffman2}
\href{https://idre.ucla.edu/hoffman2}{\tt Hoffman2} is the high-\/performance computer cluster at \href{http://www.ucla.edu}{\tt U\+C\+L\+A}. If you are using M\+O\+D\+E\+L-\/\+D\+D on Hoffman2, this page may contain useful information about the use of M\+O\+D\+E\+L.

\subparagraph*{}\hypertarget{hoffman2_hoffman2_login}{}\subsection{Login and Setup}\label{hoffman2_hoffman2_login}
\begin{DoxyVerb}ssh username@hoffman2.idre.ucla.edu
\end{DoxyVerb}


First request a c++11-\/capable compiler\+: \begin{DoxyVerb}module load gcc/4.7.2
\end{DoxyVerb}
 Then, if you are building or running the M\+P\+I version of the code you also need to load the M\+P\+I library built with gcc/4.\+7.\+2\+: \begin{DoxyVerb}module load openmpi
\end{DoxyVerb}


Alternatively, you can add the following lines to your .bashrc file \begin{DoxyVerb}module load gcc/4.7.2 > /dev/null 2>&1 
module load openmpi > /dev/null 2>&1 
\end{DoxyVerb}


\subparagraph*{}\hypertarget{hoffman2_hoffma2_dd_compile}{}\subsection{Compiling}\label{hoffman2_hoffma2_dd_compile}
Compile D\+Domp with \begin{DoxyVerb}make DDomp
\end{DoxyVerb}


Compile D\+Dmpi with \begin{DoxyVerb}make DDmpi
\end{DoxyVerb}


See also the instructions in the tutorial \hyperlink{_p_i_chargedparticles_FRsource_makefile}{Compiling\+: the Makefile}.

\subparagraph*{}\hypertarget{hoffman2_hoffma2_dd_interactive}{}\subsection{Running in interactive mode}\label{hoffman2_hoffma2_dd_interactive}
The command used to obtain an interactive section is \href{http://hpc.ucla.edu/hoffman2/computing/sge_qrsh.php}{\tt qrsh}. The command qrsh is followed by the \char`\"{}-\/l\char`\"{} directive which allows to specify a series of parameters separated by commas. For example \begin{DoxyVerb}qrsh -l h_data=4g,h_rt=2:00:00
\end{DoxyVerb}
 requests one (1) interactive processor with 4\+Gb of memory (h\+\_\+data=4g) for 2 hours, 0 minutes, and 0 seconds (h\+\_\+rt=8\+:00\+:00). The maximum time limit is 24h, unless you are a member of a resource group. In that case the time limit is 14days, but the \char`\"{}highp\char`\"{} parameter must be used\+: \begin{DoxyVerb}qrsh -l h_data=4g,h_rt=48:00:00,highp
\end{DoxyVerb}
 If your request cannot be accommodated immediately but you are willing to wait indefinitely until it is eventually honored, you can append the flag \char`\"{}-\/now n\char`\"{} to the qrsh command\+: \begin{DoxyVerb}qrsh -l h_data=4g,h_rt=48:00:00,highp -now n
\end{DoxyVerb}
 The maximum wait time for members of resource groups is 24h.

If you issue one of the previous commands, however, you still ha have one (1) processors to work with. In order to request more than one processor, we need the \char`\"{}-\/pe\char`\"{} (parallel environment) directive. The way you use the \char`\"{}-\/pe\char`\"{} directive depends on the version of the D\+D code that we run, as explained below.

\subparagraph*{}\hypertarget{hoffman2_hoffma2_dd_interactive_DDomp}{}\subsubsection{D\+Domp}\label{hoffman2_hoffma2_dd_interactive_DDomp}
If you compiled D\+Domp, \href{http://openmp.org}{\tt Open\+M\+P} is used to speed up the most computationally-\/intensive loops. Therefore, in order to achieve optimum performance, D\+Domp must be run on a shared-\/memory machine with the largest possible number of cores. In order to obtain one such machine with the qrsh command, you can use the directive \char`\"{}-\/pe shared N\char`\"{}, where \char`\"{}\+N\char`\"{} is the requested number of cores. For example\+: \begin{DoxyVerb}qrsh -l h_data=4g,h_rt=4:00:00 -pe shared 8
\end{DoxyVerb}
 requests 8 cores on the same node, which \char`\"{}share\char`\"{} the memory of that node. Note that Hoffman2 has 8-\/, 12-\/, and 16-\/core nodes.

After the node has been obtained, you can run D\+Domp as\+: \begin{DoxyVerb}./DDomp
\end{DoxyVerb}


\subparagraph*{}\hypertarget{hoffman2_hoffma2_dd_interactive_DDmpi}{}\subsubsection{D\+Dmpi}\label{hoffman2_hoffma2_dd_interactive_DDmpi}
If you compiled D\+Dmpi, \href{http://www.open-mpi.org}{\tt Open-\/\+M\+P\+I} is used in combination with \href{http://openmp.org}{\tt Open\+M\+P} to speed up the most computationally-\/intensive loops. You can then request multiple nodes (machines) on the cluster which communicate using \href{http://www.open-mpi.org}{\tt Open-\/\+M\+P\+I}, each of them internally using \href{http://openmp.org}{\tt Open\+M\+P}. Therefore, you must request \char`\"{}entire\char`\"{} nodes on the \href{https://idre.ucla.edu/hoffman2}{\tt Hoffman2} cluster, where \char`\"{}entire\char`\"{} here that all cores on a specific machine will be reserved to run D\+Dmpi. The parallel environment to be requested via qrsh is \char`\"{}-\/pe node$\ast$\char`\"{} in combination with the \char`\"{}exclusive\char`\"{} directive. For example\+: \begin{DoxyVerb}qrsh -l h_data=4g,h_rt=3:00:00,exclusive,highp -now n -pe node* 4
\end{DoxyVerb}
 requests 4 nodes, for 3 hours, and from the resource group (highp). If each node has 12 cores, the total number of cores used will be 4$\ast$12=48. D\+Dmpi will then create 4 M\+P\+I processes (on for each node), each internally creating 12 threads.

After the nodes have been obtained, it is necessary to update the environmental variables. \begin{DoxyVerb}. /u/local/bin/set_qrsh_env.sh
\end{DoxyVerb}


You can run D\+Dmpi using the \char`\"{}mpirun\char`\"{} command. However, in order to make sure that the correct version of \char`\"{}mpirun\char`\"{} is used, it is recommended to specify its full path using the environmental variable \$\+M\+P\+I\+\_\+\+B\+I\+N\+: \begin{DoxyVerb}$MPI_BIN/mpirun -pernode DDmpi
\end{DoxyVerb}
 where \char`\"{}\+N\char`\"{} is the number of requested nodes. The \char`\"{}-\/pernode\char`\"{} flag is an alternative to using a hostfile.

\subparagraph*{}\hypertarget{hoffman2_hoffma2_dd_interactive_DDmpiOLD}{}\subsubsection{D\+Dmpi (\+O\+L\+D)}\label{hoffman2_hoffma2_dd_interactive_DDmpiOLD}
If you compiled D\+Dmpi, \href{http://www.open-mpi.org}{\tt Open-\/\+M\+P\+I} is used to speed up the most computationally-\/intensive loops. Therefore, you can request processors belonging to different nodes using the \char`\"{}-\/pe dc\+\_\+$\ast$ N\char`\"{} directive, where N is the number of nodes. For example\+: \begin{DoxyVerb}qrsh -l h_data=4g,h_rt=4:00:00 -pe dc_* 96
\end{DoxyVerb}
 requests 96 processors. To use group resources (highp)\+: \begin{DoxyVerb}qrsh -l h_data=4g,h_rt=48:00:00,highp -now n -pe dc_* 96
\end{DoxyVerb}
 request 96 processors of your resource group.

After the processors have been obtained, you can run D\+Dmpi using the \char`\"{}mpirun\char`\"{} command. However, in order to make sure that the correct version of \char`\"{}mpirun\char`\"{} is used, it is recommended to specify its full path using the environmental variable \$\+M\+P\+I\+\_\+\+B\+I\+N\+: \begin{DoxyVerb}$MPI_BIN/mpirun -np N DDmpi
\end{DoxyVerb}
 where \char`\"{}\+N\char`\"{} is the number of requested processors.

\subparagraph*{}\hypertarget{hoffman2_hoffma2_dd_batch}{}\subsection{Running in batch mode}\label{hoffman2_hoffma2_dd_batch}
\subparagraph*{}\hypertarget{hoffman2_hoffma2_dd_batch_DDomp}{}\subsubsection{D\+Domp}\label{hoffman2_hoffma2_dd_batch_DDomp}
T\+O B\+E C\+O\+M\+P\+L\+E\+T\+E\+D \begin{DoxyVerb}openmp.q
\end{DoxyVerb}
 Answer the following questions
\begin{DoxyItemize}
\item 
\item 
\item 
\item answer N\+O to the
\end{DoxyItemize}

Edit the file D\+Domp.\+cmd

add the line \char`\"{}module load gcc/4.\+7.\+2\char`\"{}

\subparagraph*{}\hypertarget{hoffman2_hoffma2_dd_batch_DDmpi}{}\subsubsection{D\+Dmpi}\label{hoffman2_hoffma2_dd_batch_DDmpi}
A dedicated script has been created to run D\+Dmpi on the Hoffman2 cluster in batch mode. The script, named submit\+\_\+\+D\+Dmpi.\+sh, is listed at the bottom of this page.

Before using the script, the following should be edited
\begin{DoxyItemize}
\item h\+\_\+rt=H\+H\+H\+:\+M\+M\+:S\+S (the time requested for the run)
\item if only resource nodes are to be used, append the \char`\"{}highp\char`\"{} directive to the \char`\"{}-\/l\char`\"{} list
\end{DoxyItemize}

Once the above options have been selected, use the script as follows\+: \begin{DoxyVerb}qsub -pe node* 10 submit_DDmpi.sh
\end{DoxyVerb}
 where, in this example, 10 is the number of requested nodes. Notice that by using \char`\"{}-\/pe node$\ast$ 10\char`\"{} you are requesting 10 nodes on the cluster, each of them containing multiple cores. D\+Dmpi will then create 10 mpi processes, each of them using openmp internally to fully take advantage of multithreading.

The job should now be visible using \begin{DoxyVerb}myjob
\end{DoxyVerb}


The job can be deleted using \begin{DoxyVerb}qdel <jobID>
\end{DoxyVerb}


Following is the submit\+\_\+\+D\+Dmpi.\+sh script (thanks Raffaella D\textquotesingle{}Auria)\+: 
\begin{DoxyCodeInclude}
1 #!/bin/bash
2 
3 # change directory to work directory
4 #$ -cwd
5 
6 # define job log file
7 #$ -o parallel\_job\_penode.joblog.$JOB\_ID
8 
9 # Merged job error file error   with job log
10 #$ -j y
11 
12 # If number of nodes is not passed to qsub, uncomment following line and define number of nodes as needed
       (e.g. 5)
13 # #$ -pe node* 5
14 
15 # Request resources per node:
16 # h\_data = minimum memory per core - use at least 4Gb
17 # h\_rt = time in HHH:MM:SS
18 # exclusive = reserves the entire node (needed for openmp)
19 # highp = add highp to run on sponsor nodes
20 #$ -l h\_data=4096M,h\_rt=8:00:00,exclusive
21 
22 #  Email address to notify
23 #$ -M $USER@mail
24 #  Notify at beginning and end of job
25 #$ -m bea
26 #  Job is not rerunable
27 #$ -r n
28 
29 # echo info on joblog:
30 echo "Job $JOB\_ID started on:   "` hostname -s `
31 echo "Job $JOB\_ID started on:   "` date `
32 echo " "
33 echo "PE\_HOSTFILE: "
34 cat $PE\_HOSTFILE | awk '\{print $1" "$2\}'
35 
36 
37 # set the environment
38 . /u/local/Modules/default/init/modules.sh
39 module load gcc/4.7.2  > /dev/null 2>&1
40 module load openmpi > /dev/null 2>&1 
41 
42 # run the program
43 $MPI\_BIN/mpiexec -pernode ./DDmpi >& parallel\_job\_penode.output.$JOB\_ID
\end{DoxyCodeInclude}
 